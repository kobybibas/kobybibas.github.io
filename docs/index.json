[{"content":"TL;DR Direct Preference Optimization is a method of fine-tuning Large Language Models (LLM) to better align their outputs with human preference. It\u0026rsquo;s used as a simpler alternative to RLHF since it can be directly applied to the model without needing a reward function nor reinforcement learning optimization.\nMethod The authors propose to re-parameterize the reward model of RLHF to obtain the optimal policy in closed form. This enables to solve the standard RLHF problem using a simple classification loss.\nTo align with human preference, DPO requires the following steps:\nTrain an LLM with unsupervised data. Given a prompt, feed it twice to the LLM to generate a pair of responses. Annotate one as positive and the other as negative based on \u0026ldquo;human preference\u0026rdquo;. Train the LLM directly with the dataset of (2) with the following loss function: where y_w and y_l are the positive and negative samples. The denominator in loss function (\\phi_ref), keeps the model to not diverge too much from the original model weights. The following figure (taken from AI Coffee Break with Letitia [2]) illustrates the difference between RLHF and DPO. A side note Can this loss function be applied to a model that uses the triplet loss? It might provide better performance since it\u0026rsquo;s the closed form of the optimal policy.\nResource [1] DPO paper\n[2] AI Coffee Break with Letitia: Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n","permalink":"https://kobybibas.github.io/posts/20231223_dpo/summary/","summary":"TL;DR Direct Preference Optimization is a method of fine-tuning Large Language Models (LLM) to better align their outputs with human preference. It\u0026rsquo;s used as a simpler alternative to RLHF since it can be directly applied to the model without needing a reward function nor reinforcement learning optimization.\nMethod The authors propose to re-parameterize the reward model of RLHF to obtain the optimal policy in closed form. This enables to solve the standard RLHF problem using a simple classification loss.","title":"[Summary] Direct Preference Optimization (DPO)"},{"content":"TL;DR Machine learning models require a loss function to tune their parameters. Designing a loss function to reflect ambiguous human values poses a challenge, e.g., it\u0026rsquo;s not clear how to formulate a loss function to represent what is funny or ethical. To this end, a reward model is trained via human feedback. This reward model takes the model\u0026rsquo;s output and predicts a reward score that is then used by the model to optimize its parameters.\nMethod Pre-trained model. We are given a pre-trained model. In the RLHF context, this model was trained with a large dataset in unsupervised manner (like next token prediction). This pre-trained model then might be fine-tuned on a labeled dataset like in the summarization task. Reward model training. This stage includes:\nSample many prompts and feed them to the model. Humans rank the outputs to create a ranking score. We might use multiple models such that the same prompts going through different models. A reward model is trained based on the human feedback of (c): Given text input, its output is a scalar. Notice, this dataset is labeled and created specifically based on the pre-trained model. Therefore is much smaller than the original dataset. Fine-tune with Reinforcement Learning (RL). Here we train the model with the predictions of the reward model. However, we constrain it (the fined-tuned RLHF model) to be not too far from the original weights with the KL-divergence loss. Lastly, we plug the reward to the RL optimizer. RL update e.g. PPO. A side note: Why using a reward model and not training the base model with human feedback directly? Usually, the base model is trained in unsupervised way (like next token prediction) on a large amount of data. Annotating this data with human feedback is expensive.\nLimitations This approach is difficult to use: on one hand with don\u0026rsquo;t one the model to divergent too much from the original model to not loos performance. On the other hand, to better reflect human feedback, the model needs to be trained on the reward score.\nResource Reinforcement Learning from Human Feedback: From Zero to chatGPT\n","permalink":"https://kobybibas.github.io/posts/20231209_rlhf/summary/","summary":"TL;DR Machine learning models require a loss function to tune their parameters. Designing a loss function to reflect ambiguous human values poses a challenge, e.g., it\u0026rsquo;s not clear how to formulate a loss function to represent what is funny or ethical. To this end, a reward model is trained via human feedback. This reward model takes the model\u0026rsquo;s output and predicts a reward score that is then used by the model to optimize its parameters.","title":"[Concept] Reinforcement learning from human feedback (RLHF)"},{"content":"TL;DR Typical diffusion models create images using input text. DreamPose, presented at ECCV 2023, enhances this functionality by generating a video from an image incorporating a human model and pose sequence, as represented by DensePose.\nProblem statements Common diffusion models able to generate images based on given text. However, they can not produce animated sequence nor able to be conditioned on an input pose sequence.\nMethod Apply the following modifications to a diffusion model:\nSubstitute the CLIP text encoder with a combination of CLIP image encoder + VAE encoder + Adapter. Concatenate a pose sequence, along with random noise, to the UNET model. The rationale for step 1: Conditioning by an image can be done by concatenating the image with input noise for the denoising U-Net(as in InstructPix2Pix). However, the goal is to generate images that are not spatially aligned with the input image. For step 2, the pose conditioning is aligned with the image. Consequently, the noisy latents can be connected to the target pose representation.\nThe training procedure comprises the following steps:\nInitialize the unaltered Stable Diffusion layers from pre-trained text-to-image Stable Diffusion checkpoints. Fine-tune the UNet and adapter module on the complete training dataset (UBC Fashion) to generate frames consistent with an input image and pose. Further fine-tune the UNet and adapter module, followed by the VAE decoder, using one or more subject-specific input images to create a model tailored to the specific subject. During inference, generate video frames frame-by-frame from a single input image and a sequence of poses using the subject-specific model. Dataset The model is trained in step 2 with the UBC Fashion dataset. It contains 339 training and 100 test videos. Each video has a frame rate of 30 frames/second and is approximately 12 seconds long. During training, the authors randomly sample pairs of frames from the training videos.\nLimitations Fine-tuning the UNET, adapter, and VAE is necessary for each input image, and on V100, takes approximately one hour. The domain is highly constrained: only images featuring humans as inputs and producing solely different poses of human models as outputs. I tried running the code with Zara images. Results seem not as good as the paper figures. Proof-of-Concept Testing the method\u0026rsquo;s performance with images from Zara.\nOn the Left: the input image, on the Right: the generated video.\nThe temporal consistency and the visual similarity to the original image are not great. The reason might be that I fine-tuned the model using \u0026ldquo;\u0026ndash;use_8bit_adam\u0026rdquo; which might produce lower quality videos than the original implementation.\nResource Paper webpage\nhttps://arxiv.org/abs/2304.06025 published in ICCV 2023\nOriginal github repository\nMy adaptation of the github repository\n","permalink":"https://kobybibas.github.io/posts/20231120_dream_pose/summary/","summary":"TL;DR Typical diffusion models create images using input text. DreamPose, presented at ECCV 2023, enhances this functionality by generating a video from an image incorporating a human model and pose sequence, as represented by DensePose.\nProblem statements Common diffusion models able to generate images based on given text. However, they can not produce animated sequence nor able to be conditioned on an input pose sequence.\nMethod Apply the following modifications to a diffusion model:","title":"[Proof-of-Concept] DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"},{"content":"TL;DR A new video representation by (i) a canonical image that aggregates the static contents and (ii) a temporal deformation field that reconstructs the video frames when applied to the static image.\nProblem statements Video processing comes at a high cost,and naively processing frames results in poor cross-frame consistency.\nMethod High level objective. The proposed representations should have the following characteristics:\nFitting capability for faithful video reconstruction. Semantic correctness of the canonical image to ensure the performance of image processing algorithms. Smoothness of the deformation field to guarantee temporal consistency and correct propagation. Model. The model consists of a 2D hash-based canonical image field (C) coupled with a 3D hash-based temporal deformation field (D). An arbitrary position x in the t-th frame is first encoded by a 3D hash encoding function γ3D(x, t) to get high-dimension multi-resolution features. Then a tiny MLP maps the embedded features its corresponding position in canonical field (γ3D(x, t)) → x′.\nMultiple deformation files. For videos featuring large occlusions, additional layers corresponding to multiple content deformation fields are used. These layers would be defined based on semantic segmentation (using the Segment-Anything-track method). Each semantic layer is associated with a mask. For each layer, a group of canonical fields and deformation fields represent the separate motion of different objects.\nTraining loss. The representation is trained by minimizing the L2 loss between the ground truth color and the predicted color for a given coordinate in addition to multiple regularization terms.\nLimitations The model needs to be train per a specific video. The training duration is approximately 5 minutes for 100 video frames (3 second video).\nUnable to manage diverse videos that cannot be represented by a canonical image.\nResource https://arxiv.org/abs/2305.16311 published in ECCV 2022.\nA youtube video that shows how to run the code\nComputing methods:\nYoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Trans. Graph., 2021. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Eur. Conf. Comput. Vis., 2020. This method utilizes:\nYangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything, 2023. ","permalink":"https://kobybibas.github.io/posts/20230825_codef/summary/","summary":"TL;DR A new video representation by (i) a canonical image that aggregates the static contents and (ii) a temporal deformation field that reconstructs the video frames when applied to the static image.\nProblem statements Video processing comes at a high cost,and naively processing frames results in poor cross-frame consistency.\nMethod High level objective. The proposed representations should have the following characteristics:\nFitting capability for faithful video reconstruction. Semantic correctness of the canonical image to ensure the performance of image processing algorithms.","title":"[Summary] CoDeF: Content Deformation Fields for Temporally Consistent Video Processing"},{"content":"TL;DR This work enables interactive editing of a GAN\u0026rsquo;s generated image by translating (\u0026ldquo;dragging\u0026rdquo;) any point in the image to a target location.\nProblem statements GAN based image generation takes a noise vector to generate an image. There is a need of a localized controlled image manipulation as moving a region to a different location in the image.\nMethod Given a GAN generated image, a user input of the source coordinates (q) and the coordinates of the destination (p)\nProject the coordinates to the GAN feature space (the 6-th layer) (F(q),F(p)). Calculate the direction vector between the source and destination F(q) -\u0026gt; F(p). Finetune the GAN layers (up to the 6th layer) such that the features at the source coordinates + small translation toward the destination coordinates be equal to the source features. Repeat 1-3 until reaching the destination location. An additional contribution is: instead using a tracker to track the movement of the source features toward the target destination that might add an additional computation cost, they track the movement of the source features directly using nearest neighbor with the GAN\u0026rsquo;s feature space.\nLimitations This method enables the manipulation of a GAN\u0026rsquo;s generated image only thus a user cannot feed it their own images.\nhttps://arxiv.org/abs/2305.10973\n","permalink":"https://kobybibas.github.io/posts/20231014_drage_your_gan/summary/","summary":"TL;DR This work enables interactive editing of a GAN\u0026rsquo;s generated image by translating (\u0026ldquo;dragging\u0026rdquo;) any point in the image to a target location.\nProblem statements GAN based image generation takes a noise vector to generate an image. There is a need of a localized controlled image manipulation as moving a region to a different location in the image.\nMethod Given a GAN generated image, a user input of the source coordinates (q) and the coordinates of the destination (p)","title":"[Summary] Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold"},{"content":"TL;DR Fine-tuning of a diffusion model using a single image to generate images conditions on user-provided concepts.\nProblem statements Diffusion models are not able to generate a new image of user-provided concepts. Methods (DreemBooth) that enable this capabilities require several input images that contain the desired concept.\nMethod The method consists of two phases.\nFreezing the model weights, and optimize handles to reconstruct the input image. This is done with a large learning rate to not harm the model generalization. Fine-tuning the model weights while continuing to optimize the handles. This is done with a small learning rate that enables faithful reconstruction of the extracted concepts. In addition, the authors incorporate the following losses: Union sampling: The resulting model struggles to generate images the combine several concepts. To address it, they train the model with prompts like: “a photo of [v_1] and . . . [v_2]”.\nTo ensure each handle is associated with only a single concept they combine two losses:\nMasked loss: The handles and model weights are optimized using a masked version of the diffusion loss, i.e., by penalizing only over the pixels covered by the concept masks. This makes sure each handle is associated with one mask (eq. 1 in the paper). Attention loss: They utilize the cross-attention maps for the newly-added tokens and penalize their MSE deviation from the input masks. Limitations Personalization process is slow: each image requires training of the diffusion model (from the paper \u0026ldquo;our method takes about 4.5 minutes to extract the concepts from a single scene and to fine-tune the entire model.\u0026rdquo;)\nThe style of the image is indistinguishable from the objects. From the authors\u0026rsquo; example: when an image with daylight was introduced, all generated images included daylight.\nFrom the paper examples, seems like the pose is fixed: The dog in figure 10 is always in the same pose.\nIn their evaluation, they used COCO annotated segmentation masks. It is not clear how much noisy segmentation masks affect the performance. This is important for a large scale image generation.\nResource https://arxiv.org/abs/2305.16311\n","permalink":"https://kobybibas.github.io/posts/20230721_break_a_scene/summary/","summary":"TL;DR Fine-tuning of a diffusion model using a single image to generate images conditions on user-provided concepts.\nProblem statements Diffusion models are not able to generate a new image of user-provided concepts. Methods (DreemBooth) that enable this capabilities require several input images that contain the desired concept.\nMethod The method consists of two phases.\nFreezing the model weights, and optimize handles to reconstruct the input image. This is done with a large learning rate to not harm the model generalization.","title":"[Summary] Break-A-Scene: Extracting Multiple Concepts from a Single Image"},{"content":"TL;DR To enable a more controllable image diffusion, MultiDiffusion introduce patches generation with a global constrain.\nProblem statements Diffusion models lack user controllability and methods that offer such control require a costly fine-tuning.\nMethod The method can be reduced to the following algorithm: At each time step t:\nExtract patches from the global image I_{t-1} Execute the de-noising step to generate the patches J_{i,t} Combine the patches by average their pixel values to create the global image I_t For the panorama use case: simply generate N images with overlapping regions between them. At each de-noising step, take the average pixel values of the overlapping regions.\nLimitations The compute cost is linear in the number of patches (each additional patch require diffusion model inference)\nResource https://arxiv.org/abs/2302.08113\n","permalink":"https://kobybibas.github.io/posts/20230525_multidiffusion/summary/","summary":"TL;DR To enable a more controllable image diffusion, MultiDiffusion introduce patches generation with a global constrain.\nProblem statements Diffusion models lack user controllability and methods that offer such control require a costly fine-tuning.\nMethod The method can be reduced to the following algorithm: At each time step t:\nExtract patches from the global image I_{t-1} Execute the de-noising step to generate the patches J_{i,t} Combine the patches by average their pixel values to create the global image I_t For the panorama use case: simply generate N images with overlapping regions between them.","title":"[Summary] MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation"},{"content":"","permalink":"https://kobybibas.github.io/posts/","summary":"posts","title":"Posts"},{"content":"Koby Bibas, Shachar Shayovitz, and Meir Feder, ``Deep Active Learning for Individual Data\u0026rsquo;\u0026rsquo;. Under review, 2023.\nKoby Bibas, and Meir Feder, ``Beyond Ridge Regression for Distribution-Free Data\u0026rsquo;\u0026rsquo;. Preprint arXiv:2206.08757, 2022.\nKoby Bibas, Meir Feder, and Tal Hassner, ``Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection\u0026rsquo;\u0026rsquo;. The thirty-fifth Conference on Neural Information Processing Systems (NeurIPS), 2021.\nKoby Bibas, and Meir Feder, ``Distribution Free Uncertainty for the Minimum Norm Solution of Over-parameterized Linear Regression\u0026rsquo;\u0026rsquo;. Workshop on Distribution-Free Uncertainty Quantification ICML, 2021.\nKoby Bibas, Gili Weiss-Dicker, Dana Cohen, Noa Cahan, and Hayit Greenspan, ``Learning Rotation Invariant Features for Cryogenic Electron Microscopy Image Reconstruction\u0026rsquo;\u0026rsquo;. The International Symposium on Biomedical Imaging (ISBI), 2021.\nKoby Bibas, Yaniv Fogel, and Meir Feder, ``New look at an old problem: A universal learning approach to linear regression\u0026rsquo;\u0026rsquo;. The IEEE International Symposium on Information Theory (ISIT), 2019.\n","permalink":"https://kobybibas.github.io/publications/","summary":"publications","title":"Publications"}]