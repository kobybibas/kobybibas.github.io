---
title: "[Summary] Direct Preference Optimization (DPO)"
date: 2023-12-23
tags: 
    - Reinforcement learning
    - LLM
    - Reward function
draft: false
---

## TL;DR
As an alternative to RLHF, optimization can be directly applied to the model without reward model nor reinforcement learning optimization.


## Method


![DPO loss function](/posts/20231223_dpo/dpo_loss_function.png)
![DPO update step](/posts/20231223_dpo/dpo_update_step.png)


## RLHF VS DPO

![DPO loss function](/posts/20231223_dpo/rlhf_vs_dpo.png)



## Limitations


## Resource

[AI Coffee Break with Letitia: Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://www.youtube.com/watch?v=XZLc09hkMwA)

[DPO paper](https://arxiv.org/abs/2305.18290)
